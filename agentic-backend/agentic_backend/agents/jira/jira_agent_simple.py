import logging
import os

from langchain.agents import create_agent
from langchain.tools import tool
from langchain_core.messages import AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langfuse.langchain import CallbackHandler as LangfuseCallbackHandler
from langgraph.graph.state import CompiledStateGraph

from agentic_backend.application_context import get_default_chat_model
from agentic_backend.common.mcp_runtime import MCPRuntime
from agentic_backend.common.structures import AgentChatOptions
from agentic_backend.core.agents.agent_flow import AgentFlow
from agentic_backend.core.agents.agent_spec import (
    AgentTuning,
    FieldSpec,
    MCPServerRef,
    UIHints,
)
from agentic_backend.core.agents.runtime_context import RuntimeContext
from agentic_backend.core.runtime_source import expose_runtime_source

logger = logging.getLogger(__name__)

# ---------------------------
# Tuning spec (UI-editable)
# ---------------------------
TUNING = AgentTuning(
    role="Jira backlog and test builder",
    description="Extracts requirements and user stories from project documents to fill a Jira board and build Zephyr tests.",
    mcp_servers=[MCPServerRef(name="mcp-knowledge-flow-mcp-text")],
    tags=[],
    fields=[
        FieldSpec(
            key="prompts.system",
            type="prompt",
            title="System Prompt",
            description="You extract requirements, user stories and build tests from project documents",  # to fill a Jira board and build Zephyr tests.",
            required=True,
            default="""
Tu es un Business Analyst et Product Owner expert avec accÃ¨s Ã  des outils spÃ©cialisÃ©s.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OUTILS DISPONIBLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tu disposes de 4 types d'outils :

1. **Outils de recherche documentaire (MCP)** :
   - UtilisÃ©s pour extraire des informations des documents projet (.docx, .pdf, etc.)
   - Exemple : search_documents, get_document_content, etc.

2. **generate_requirements** :
   - GÃ©nÃ¨re une liste d'exigences formelles (fonctionnelles et non-fonctionnelles)
   - IMPORTANT : Cet outil fait un appel LLM sÃ©parÃ©, donc ne timeout pas
   - Retourne un message confirmant que les exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es

3. **generate_user_stories** :
   - GÃ©nÃ¨re des User Stories avec critÃ¨res d'acceptation Gherkin exhaustifs
   - IMPORTANT : Cet outil fait un appel LLM sÃ©parÃ©, donc ne timeout pas
   - Peut recevoir les exigences en paramÃ¨tre pour assurer la cohÃ©rence
   - Retourne un message confirmant que les user stories ont Ã©tÃ© gÃ©nÃ©rÃ©es

4. **generate_tests** :
   - GÃ©nÃ¨re des scÃ©narios de tests dÃ©taillÃ©s au format Gherkin
   - IMPORTANT : Cet outil fait un appel LLM sÃ©parÃ©, donc ne timeout pas
   - NÃ©cessite les User Stories en paramÃ¨tre
   - Peut recevoir un JDD (Jeu de DonnÃ©es) optionnel pour les personas
   - Retourne un message confirmant que les scÃ©narios de tests ont Ã©tÃ© gÃ©nÃ©rÃ©s

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WORKFLOW RECOMMANDÃ‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Ã‰tape 1 : Extraction du contexte projet**
- Utilise les outils MCP pour rechercher et extraire les informations des documents
- Effectue plusieurs recherches ciblÃ©es pour couvrir diffÃ©rents aspects
- Prends des notes sur ce que tu trouves

**Ã‰tape 2 : GÃ©nÃ©ration des exigences (si demandÃ©)**
- Appelle generate_requirements(context_summary="[rÃ©sumÃ© de ce que tu as trouvÃ©]")
- L'outil gÃ©nÃ¨re les exigences et retourne un message de confirmation

**Ã‰tape 3 : GÃ©nÃ©ration des User Stories (si demandÃ©)**
- Si l'utilisateur demande aussi des user stories :
  - Appelle generate_user_stories(context_summary="...", requirements="[exigences de l'Ã©tape 2]")
- Si l'utilisateur demande UNIQUEMENT des user stories :
  - Appelle directement generate_user_stories(context_summary="...", requirements="")
- L'outil gÃ©nÃ¨re les user stories et retourne un message de confirmation

**Ã‰tape 4 : GÃ©nÃ©ration des scÃ©narios de tests (si demandÃ©)**
- Si l'utilisateur demande des tests :
  - Appelle generate_tests(user_stories="[user stories de l'Ã©tape 3]", jdd="[JDD si fourni]")
- L'outil gÃ©nÃ¨re les scÃ©narios de tests et retourne un message de confirmation

**Ã‰tape 5 : Conclusion**
- Une fois tous les outils appelÃ©s, termine ta rÃ©ponse
- Les rÃ©sultats dÃ©taillÃ©s (exigences, user stories et tests) seront automatiquement affichÃ©s Ã  l'utilisateur""",
            ui=UIHints(group="Prompts", multiline=True, markdown=True),
        ),
        FieldSpec(
            key="chat_options.attach_files",
            type="boolean",
            title="Allow file attachments",
            description="Show file upload/attachment controls for this agent.",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.libraries_selection",
            type="boolean",
            title="Document libraries picker",
            description="Let users select document libraries/knowledge sources for this agent.",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.search_policy_selection",
            type="boolean",
            title="Search policy selector",
            description="Expose the search policy toggle (hybrid/semantic/strict).",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.search_rag_scoping",
            type="boolean",
            title="RAG scope selector",
            description="Expose the RAG scope control (documents-only vs hybrid vs knowledge).",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.deep_search_delegate",
            type="boolean",
            title="Deep search delegate toggle",
            description="Allow delegation to a senior agent for deep search.",
            required=False,
            default=False,
            ui=UIHints(group="Chat options"),
        ),
    ],
)


@expose_runtime_source("agent.Jim")
class JiraAgent(AgentFlow):
    tuning = TUNING
    default_chat_options = AgentChatOptions(
        attach_files=True,
        libraries_selection=True,
        search_rag_scoping=True,
        search_policy_selection=True,
        deep_search_delegate=False,
    )

    async def async_init(self, runtime_context: RuntimeContext):
        await super().async_init(runtime_context=runtime_context)
        self.mcp = MCPRuntime(agent=self)
        await self.mcp.init()
        # Storage for tool outputs
        self.generated_requirements = None
        self.generated_user_stories = None
        self.generated_tests = None
        # Check if Langfuse is configured
        self.langfuse_enabled = bool(
            os.getenv("LANGFUSE_PUBLIC_KEY") and os.getenv("LANGFUSE_SECRET_KEY")
        )
        if self.langfuse_enabled:
            logger.info("[JiraAgent] Langfuse tracing enabled")

    def _get_langfuse_handler(self) -> LangfuseCallbackHandler | None:
        """Create a Langfuse callback handler for tracing LLM calls."""
        if not self.langfuse_enabled:
            return None
        return LangfuseCallbackHandler()

    async def aclose(self):
        await self.mcp.aclose()

    def get_requirements_tool(self):
        """Tool that generates requirements using a separate LLM call"""

        # Capture self reference for closure
        agent_self = self

        @tool
        async def generate_requirements(context_summary: str) -> str:
            """
            GÃ©nÃ¨re une liste d'exigences formelles (fonctionnelles et non-fonctionnelles)
            Ã  partir du contexte projet fourni par les recherches documentaires.

            IMPORTANT: Avant d'appeler cet outil, utilise les outils de recherche MCP
            pour extraire les informations pertinentes des documents projet, puis
            fournis un rÃ©sumÃ© de ce contexte en paramÃ¨tre.

            Args:
                context_summary: RÃ©sumÃ© du contexte projet extrait des documents

            Returns:
                Message de confirmation que les exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es
            """
            requirements_prompt = """
Tu es un Business Analyst expert. GÃ©nÃ¨re une liste d'exigences formelles basÃ©e sur le contexte projet suivant.

Contexte projet extrait des documents:
{context_summary}

Consignes :
1. **GÃ©nÃ¨re des exigences fonctionnelles et non-fonctionnelles**
2. **Formalisme :** Exigences claires, concises, non ambiguÃ«s et testables
3. **ID Unique :** Ex: EX-FON-001 (fonctionnelle), EX-NFON-001 (non-fonctionnelle)
4. **Priorisation :** Haute, Moyenne ou Basse

Format attendu pour chaque exigence:
- ID: [ID unique]
- Titre: [Nom concis]
- Description: [Exigence dÃ©taillÃ©e]
- Type: [Fonctionnelle/Non-fonctionnelle]
- PrioritÃ©: [Haute/Moyenne/Basse]
"""

            model = get_default_chat_model()
            messages = [
                SystemMessage(
                    content=requirements_prompt.format(context_summary=context_summary)
                )
            ]

            # Add Langfuse tracing if enabled
            langfuse_handler = agent_self._get_langfuse_handler()
            config: RunnableConfig = (
                {"callbacks": [langfuse_handler]} if langfuse_handler else {}
            )

            response = await model.ainvoke(messages, config=config)

            # Store the full output
            content = response.content
            if isinstance(content, str):
                agent_self.generated_requirements = content
            elif isinstance(content, list):
                agent_self.generated_requirements = "".join(
                    part if isinstance(part, str) else part.get("text", "")
                    for part in content
                )
            else:
                agent_self.generated_requirements = str(content)

            # Return confirmation message
            return "âœ“ Exigences gÃ©nÃ©rÃ©es avec succÃ¨s. Elles seront affichÃ©es Ã  la fin de la conversation."

        return generate_requirements

    def get_user_stories_tool(self):
        """Tool that generates user stories using a separate LLM call"""

        # Capture self reference for closure
        agent_self = self

        @tool
        async def generate_user_stories(
            context_summary: str, requirements: str = ""
        ) -> str:
            """
            GÃ©nÃ¨re des User Stories de haute qualitÃ© avec critÃ¨res d'acceptation exhaustifs (Gherkin).

            IMPORTANT: Avant d'appeler cet outil, utilise les outils de recherche MCP
            pour extraire les informations pertinentes des documents projet.

            Args:
                context_summary: RÃ©sumÃ© du contexte projet extrait des documents
                requirements: Les exigences prÃ©alablement gÃ©nÃ©rÃ©es (optionnel, pour cohÃ©rence)

            Returns:
                Message de confirmation que les user stories ont Ã©tÃ© gÃ©nÃ©rÃ©es
            """
            stories_prompt = """
Tu es un Product Owner expert. GÃ©nÃ¨re des User Stories de haute qualitÃ©.

Contexte projet extrait des documents:
{context_summary}

{requirements_section}

**Structure de base :**
- **Format :** "En tant que [persona], je veux [action], afin de [bÃ©nÃ©fice]"
- **ID Unique :** Ex: US-001, US-002
- Stories atomiques, verticales et testables
- **CohÃ©rence :** Couvre les exigences si elles sont fournies
- **Couverture complÃ¨te :** Happy path + cas d'erreur + tous les personas

**CritÃ¨res d'Acceptation Exhaustifs (Format Gherkin)** - OBLIGATOIRE pour CHAQUE story :

1. **Cas Nominaux (Happy Path) :**
   - ScÃ©nario idÃ©al oÃ¹ tout fonctionne

2. **Validations de DonnÃ©es :**
   - Formats invalides (email, mot de passe, etc.)
   - Champs obligatoires manquants
   - Limites min/max de caractÃ¨res
   - Fichiers non supportÃ©s ou trop volumineux
   - UnicitÃ© des donnÃ©es (doublons)

3. **Cas d'Erreur :**
   - Erreurs techniques (API, timeout, erreur 500)
   - Erreurs mÃ©tier (stock insuffisant, droits insuffisants)
   - Perte de connexion

4. **Cas Limites :**
   - Valeurs frontiÃ¨res (0, 1, max, max+1)
   - Listes vides ou trÃ¨s longues
   - Dates limites (29 fÃ©vrier, changement d'heure)

5. **Feedback Utilisateur :**
   - Messages de succÃ¨s EXACTS (Toasts, Modales)
   - Messages d'erreur EXACTS affichÃ©s
   - Ã‰tats de chargement et boutons dÃ©sactivÃ©s

**Format Gherkin strict :** "Ã‰tant donnÃ© que [contexte], Quand [action], Alors [rÃ©sultat attendu]"

**MÃ©tadonnÃ©es :**
- **Estimation :** Fibonacci (1, 2, 3, 5, 8, 13, 21)
- **Priorisation :** Must Have, Should Have, Could Have, Won't Have
- **DÃ©pendances :** Ordre logique, AUCUNE dÃ©pendance circulaire
- **Questions :** 1 Ã  3 questions de clarification par story
"""

            requirements_section = ""
            if requirements:
                requirements_section = f"""
Exigences Ã  respecter:
{requirements}
"""

            model = get_default_chat_model()
            messages = [
                SystemMessage(
                    content=stories_prompt.format(
                        context_summary=context_summary,
                        requirements_section=requirements_section,
                    )
                )
            ]

            # Add Langfuse tracing if enabled
            langfuse_handler = agent_self._get_langfuse_handler()
            config: RunnableConfig = (
                {"callbacks": [langfuse_handler]} if langfuse_handler else {}
            )

            response = await model.ainvoke(messages, config=config)

            # Store the full output
            content = response.content
            if isinstance(content, str):
                agent_self.generated_user_stories = content
            elif isinstance(content, list):
                agent_self.generated_user_stories = "".join(
                    part if isinstance(part, str) else part.get("text", "")
                    for part in content
                )
            else:
                agent_self.generated_user_stories = str(content)

            # Return confirmation message
            return "âœ“ User Stories gÃ©nÃ©rÃ©es avec succÃ¨s. Elles seront affichÃ©es Ã  la fin de la conversation."

        return generate_user_stories

    def get_tests_tool(self):
        """Tool that generates test scenarios using a separate LLM call"""

        # Capture self reference for closure
        agent_self = self

        @tool
        async def generate_tests(user_stories: str, jdd: str = "") -> str:
            """
            GÃ©nÃ¨re des scÃ©narios de tests dÃ©taillÃ©s et exploitables Ã  partir des User Stories fournies.

            IMPORTANT: Avant d'appeler cet outil, assure-toi d'avoir les User Stories.
            Tu peux Ã©galement fournir un JDD (Jeu de DonnÃ©es) pour les personas de chaque test.

            Args:
                user_stories: Les User Stories Ã  couvrir par les tests
                jdd: Jeu de DonnÃ©es pour les personas (optionnel)

            Returns:
                Message de confirmation que les scÃ©narios de tests ont Ã©tÃ© gÃ©nÃ©rÃ©s
            """
            tests_prompt = """
## RÃ´le

Tu es un expert en tests logiciels. Ton rÃ´le est de crÃ©er des scÃ©narios de tests dÃ©taillÃ©s et exploitables.

## Instructions principales

GÃ©nÃ¨re des scÃ©narios de tests complets Ã  partir des informations fournies dans les User Stories (US) suivantes, en suivant le format Gherkin (Etant donnÃ© que-Lorsque-Alors) et en incluant les cas nominaux, limites et d'erreur. Toutes les US fournies doivent faire l'objet d'un test.
Tu peux Ã©galement te baser sur les JDDs fournis en entrÃ©e pour les personas de chaque tests

## Format de rÃ©ponse attendu ðŸ“

Pour chaque scÃ©nario :

1. **ID du ScÃ©nario** : Un identifiant unique (ex: SC-001, SC-LOGIN-001).
2. **userStoryId**: L'ID de la User Story couverte par ce test.
3. **Titre du ScÃ©nario** : Un titre concis dÃ©crivant l'objectif du test.
4. **Description** : Une brÃ¨ve explication de ce que le scÃ©nario teste.
5. **PrÃ©conditions** : Les Ã©tats ou donnÃ©es nÃ©cessaires avant l'exÃ©cution du test.
6. **Ã‰tapes** : Au format Gherkin prÃ©sentÃ©es sous forme de tableau avec les colonnes suivantes : NumÃ©ro (#1, #2, ...), Action (Etant donnÃ© que - Lorsque), RÃ©sultat attendu (Alors).
7. **DonnÃ©es de test** : Jeux de donnÃ©es nÃ©cessaires
8. **PrioritÃ©** : (Haute, Moyenne, Basse) Indiquant l'importance du test.
9. **type**: Le type de cas de test (Nominal, Limite, Erreur).

-------------------------------------------

**--- DÃ‰BUT DES USER STORIES Ã€ ANALYSER ---**
{USER_STORIES}
**--- FIN DES USER STORIES Ã€ ANALYSER ---**

**--- DÃ‰BUT DU JDD Ã€ ANALYSER ---**
{JDD}
**--- FIN DU JDD Ã€ ANALYSER ---**
"""

            model = get_default_chat_model()
            messages = [
                SystemMessage(
                    content=tests_prompt.format(
                        USER_STORIES=user_stories,
                        JDD=jdd if jdd else "Aucun JDD fourni",
                    )
                )
            ]

            # Add Langfuse tracing if enabled
            langfuse_handler = agent_self._get_langfuse_handler()
            config: RunnableConfig = (
                {"callbacks": [langfuse_handler]} if langfuse_handler else {}
            )

            response = await model.ainvoke(messages, config=config)

            # Store the full output
            content = response.content
            if isinstance(content, str):
                agent_self.generated_tests = content
            elif isinstance(content, list):
                agent_self.generated_tests = "".join(
                    part if isinstance(part, str) else part.get("text", "")
                    for part in content
                )
            else:
                agent_self.generated_tests = str(content)

            # Return confirmation message
            return "âœ“ ScÃ©narios de tests gÃ©nÃ©rÃ©s avec succÃ¨s. Ils seront affichÃ©s Ã  la fin de la conversation."

        return generate_tests

    async def astream_updates(self, state, *, config=None, **kwargs):
        """Override to append stored tool outputs to final response"""
        final_event = None

        # Stream all events from parent
        async for event in super().astream_updates(state, config=config, **kwargs):
            final_event = event
            yield event

        # After streaming is complete, if we have stored outputs, send them as additional messages
        if final_event is not None and (
            self.generated_requirements
            or self.generated_user_stories
            or self.generated_tests
        ):
            # Build the additional content
            additional_content = "\n\n---\n\n"

            if self.generated_requirements:
                additional_content += "# ðŸ“‹ Exigences GÃ©nÃ©rÃ©es\n\n"
                additional_content += self.generated_requirements
                additional_content += "\n\n"

            if self.generated_user_stories:
                additional_content += "# ðŸ“ User Stories GÃ©nÃ©rÃ©es\n\n"
                additional_content += self.generated_user_stories
                additional_content += "\n\n"

            if self.generated_tests:
                additional_content += "# ðŸ§ª ScÃ©narios de Tests GÃ©nÃ©rÃ©s\n\n"
                additional_content += self.generated_tests

            # Create a new AI message with the stored content
            additional_message = AIMessage(content=additional_content)

            # Yield it as a new update
            yield {"agent": {"messages": [additional_message]}}

            # Reset for next run
            self.generated_requirements = None
            self.generated_user_stories = None
            self.generated_tests = None

    def get_compiled_graph(self) -> CompiledStateGraph:
        requirements_tool = self.get_requirements_tool()
        user_stories_tool = self.get_user_stories_tool()
        tests_tool = self.get_tests_tool()

        return create_agent(
            model=get_default_chat_model(),
            system_prompt=self.render(self.get_tuned_text("prompts.system") or ""),
            tools=[
                requirements_tool,
                user_stories_tool,
                tests_tool,
                *self.mcp.get_tools(),
            ],
            checkpointer=self.streaming_memory,
        )
