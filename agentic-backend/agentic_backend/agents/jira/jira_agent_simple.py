import csv
import io
import logging
import os
import re
import tempfile
from datetime import datetime
from pathlib import Path

from langchain.agents import AgentState, create_agent
from langchain.messages import ToolMessage
from langchain.tools import ToolRuntime, tool
from langchain_core.messages import SystemMessage
from langchain_core.runnables import RunnableConfig
from langfuse.langchain import CallbackHandler as LangfuseCallbackHandler
from langgraph.graph.state import CompiledStateGraph
from langgraph.types import Command

from agentic_backend.application_context import get_default_chat_model
from agentic_backend.common.mcp_runtime import MCPRuntime
from agentic_backend.common.structures import AgentChatOptions
from agentic_backend.core.agents.agent_flow import AgentFlow
from agentic_backend.core.agents.agent_spec import (
    AgentTuning,
    FieldSpec,
    MCPServerRef,
    UIHints,
)
from agentic_backend.core.agents.runtime_context import RuntimeContext
from agentic_backend.core.chatbot.chat_schema import LinkKind, LinkPart
from agentic_backend.core.runtime_source import expose_runtime_source

logger = logging.getLogger(__name__)


class CustomState(AgentState):
    generated_requirements: str
    generated_user_stories: str
    generated_user_stories_jira: str  # JSON format for Jira CSV export
    generated_tests: str


# ---------------------------
# Tuning spec (UI-editable)
# ---------------------------
TUNING = AgentTuning(
    role="Jira backlog and test builder",
    description="Extracts requirements and user stories from project documents to fill a Jira board and build Zephyr tests.",
    mcp_servers=[MCPServerRef(name="mcp-knowledge-flow-mcp-text")],
    tags=[],
    fields=[
        FieldSpec(
            key="prompts.system",
            type="prompt",
            title="System Prompt",
            description="You extract requirements, user stories and build tests from project documents",  # to fill a Jira board and build Zephyr tests.",
            required=True,
            default="""
Tu es un Business Analyst et Product Owner expert avec accÃ¨s Ã  des outils spÃ©cialisÃ©s.
Ton but est de gÃ©nÃ©rer des exigences formelles, user stories et/ou cas de tests pour un projet selon la demande de l'utilisateur.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OUTILS DISPONIBLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tu disposes de 7 types d'outils :

1. **Outils de recherche documentaire (MCP)** :
   - UtilisÃ©s pour extraire des informations des documents projet (.docx, .pdf, etc.)
   - Exemple : search_documents, get_document_content, etc.

2. **generate_requirements** :
   - GÃ©nÃ¨re une liste d'exigences formelles (fonctionnelles et non-fonctionnelles)
   - IMPORTANT : Cet outil fait un appel LLM sÃ©parÃ©, donc ne timeout pas
   - Retourne un message confirmant que les exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es

3. **generate_user_stories** :
   - GÃ©nÃ¨re des User Stories avec critÃ¨res d'acceptation Gherkin exhaustifs
   - IMPORTANT : Cet outil fait un appel LLM sÃ©parÃ©, donc ne timeout pas
   - CHAÃŽNAGE AUTOMATIQUE : Si des exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es avant, elles sont automatiquement utilisÃ©es
   - Retourne un message confirmant que les user stories ont Ã©tÃ© gÃ©nÃ©rÃ©es

4. **generate_tests** :
   - GÃ©nÃ¨re des scÃ©narios de tests dÃ©taillÃ©s au format Gherkin
   - IMPORTANT : Cet outil fait un appel LLM sÃ©parÃ©, donc ne timeout pas
   - CHAÃŽNAGE AUTOMATIQUE : Utilise automatiquement les User Stories gÃ©nÃ©rÃ©es prÃ©cÃ©demment
   - Peut recevoir un JDD (Jeu de DonnÃ©es) optionnel pour les personas
   - Retourne un message confirmant que les scÃ©narios de tests ont Ã©tÃ© gÃ©nÃ©rÃ©s

5. **export_deliverables** :
   - Exporte tous les livrables gÃ©nÃ©rÃ©s (exigences, user stories, tests) dans un fichier Markdown
   - Retourne un lien de tÃ©lÃ©chargement pour l'utilisateur
   - OBLIGATOIRE : Appelle cet outil Ã  la fin pour fournir le fichier Ã  l'utilisateur

6. **generate_user_stories_for_jira** :
   - GÃ©nÃ¨re des User Stories au format JSON structurÃ© pour l'import Jira
   - IMPORTANT : Cet outil fait un appel LLM sÃ©parÃ©, donc ne timeout pas
   - CHAÃŽNAGE AUTOMATIQUE : Si des exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es avant, elles sont automatiquement utilisÃ©es
   - Utilise cet outil Ã  la place de generate_user_stories si l'utilisateur veut importer dans Jira

7. **export_jira_csv** :
   - Exporte les User Stories Jira gÃ©nÃ©rÃ©es dans un fichier CSV compatible avec l'import Jira
   - IMPORTANT : NÃ©cessite d'avoir appelÃ© generate_user_stories_for_jira au prÃ©alable
   - Retourne un lien de tÃ©lÃ©chargement du fichier CSV

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WORKFLOW RECOMMANDÃ‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Ã‰tape 1 : Extraction du contexte projet**
- Utilise les outils MCP pour rechercher et extraire les informations des documents
- Effectue plusieurs recherches ciblÃ©es pour couvrir diffÃ©rents aspects
- Prends des notes sur ce que tu trouves

**Ã‰tape 2 : GÃ©nÃ©ration des exigences (si demandÃ©)**
- Appelle generate_requirements(context_summary="[rÃ©sumÃ© de ce que tu as trouvÃ©]")
- L'outil gÃ©nÃ¨re les exigences et retourne un message de confirmation

**Ã‰tape 3 : GÃ©nÃ©ration des User Stories (si demandÃ©)**
- Appelle generate_user_stories(context_summary="[rÃ©sumÃ© de ce que tu as trouvÃ©]")
- Les exigences gÃ©nÃ©rÃ©es Ã  l'Ã©tape 2 sont automatiquement utilisÃ©es si disponibles
- L'outil gÃ©nÃ¨re les user stories et retourne un message de confirmation

**Ã‰tape 4 : GÃ©nÃ©ration des scÃ©narios de tests (si demandÃ©)**
- Appelle generate_tests() ou generate_tests(jdd="[JDD si fourni]")
- Les User Stories gÃ©nÃ©rÃ©es Ã  l'Ã©tape 3 sont automatiquement utilisÃ©es
- IMPORTANT : generate_user_stories doit avoir Ã©tÃ© appelÃ© avant
- L'outil gÃ©nÃ¨re les scÃ©narios de tests et retourne un message de confirmation

**Ã‰tape 5 : Export des livrables (OBLIGATOIRE)**
- Appelle export_deliverables() pour gÃ©nÃ©rer le fichier Markdown tÃ©lÃ©chargeable
- Cet outil retourne un lien de tÃ©lÃ©chargement Ã  prÃ©senter Ã  l'utilisateur

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WORKFLOW ALTERNATIF : EXPORT JIRA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Si l'utilisateur souhaite importer les User Stories dans Jira :

**Ã‰tape 1 : Extraction du contexte projet** (identique)

**Ã‰tape 2 : GÃ©nÃ©ration des exigences** (optionnel, identique)

**Ã‰tape 3 : GÃ©nÃ©ration des User Stories pour Jira**
- Appelle generate_user_stories_for_jira(context_summary="[rÃ©sumÃ©]") au lieu de generate_user_stories
- GÃ©nÃ¨re des User Stories au format JSON structurÃ©

**Ã‰tape 4 : Export CSV Jira**
- Appelle export_jira_csv() pour gÃ©nÃ©rer le fichier CSV tÃ©lÃ©chargeable
- Ce fichier est directement importable dans Jira via Project settings > External system import""",
            ui=UIHints(group="Prompts", multiline=True, markdown=True),
        ),
        FieldSpec(
            key="chat_options.attach_files",
            type="boolean",
            title="Allow file attachments",
            description="Show file upload/attachment controls for this agent.",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.libraries_selection",
            type="boolean",
            title="Document libraries picker",
            description="Let users select document libraries/knowledge sources for this agent.",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.search_policy_selection",
            type="boolean",
            title="Search policy selector",
            description="Expose the search policy toggle (hybrid/semantic/strict).",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.search_rag_scoping",
            type="boolean",
            title="RAG scope selector",
            description="Expose the RAG scope control (documents-only vs hybrid vs knowledge).",
            required=False,
            default=True,
            ui=UIHints(group="Chat options"),
        ),
        FieldSpec(
            key="chat_options.deep_search_delegate",
            type="boolean",
            title="Deep search delegate toggle",
            description="Allow delegation to a senior agent for deep search.",
            required=False,
            default=False,
            ui=UIHints(group="Chat options"),
        ),
    ],
)


@expose_runtime_source("agent.Jim")
class JiraAgent(AgentFlow):
    tuning = TUNING
    default_chat_options = AgentChatOptions(
        attach_files=True,
        libraries_selection=True,
        search_rag_scoping=True,
        search_policy_selection=True,
        deep_search_delegate=False,
    )

    async def async_init(self, runtime_context: RuntimeContext):
        await super().async_init(runtime_context=runtime_context)
        self.mcp = MCPRuntime(agent=self)
        await self.mcp.init()
        # Check if Langfuse is configured
        self.langfuse_enabled = bool(
            os.getenv("LANGFUSE_PUBLIC_KEY") and os.getenv("LANGFUSE_SECRET_KEY")
        )
        if self.langfuse_enabled:
            logger.info("[JiraAgent] Langfuse tracing enabled")

    def _get_langfuse_handler(self) -> LangfuseCallbackHandler | None:
        """Create a Langfuse callback handler for tracing LLM calls."""
        if not self.langfuse_enabled:
            return None
        return LangfuseCallbackHandler()

    async def aclose(self):
        await self.mcp.aclose()

    def get_requirements_tool(self):
        """Tool that generates requirements using a separate LLM call"""

        @tool
        async def generate_requirements(runtime: ToolRuntime, context_summary: str):
            """
            GÃ©nÃ¨re une liste d'exigences formelles (fonctionnelles et non-fonctionnelles)
            Ã  partir du contexte projet fourni par les recherches documentaires.

            IMPORTANT: Avant d'appeler cet outil, utilise les outils de recherche MCP
            pour extraire les informations pertinentes des documents projet, puis
            fournis un rÃ©sumÃ© de ce contexte en paramÃ¨tre.

            Args:
                context_summary: RÃ©sumÃ© du contexte projet extrait des documents

            Returns:
                Message de confirmation que les exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es
            """
            requirements_prompt = """
Tu es un Business Analyst expert. GÃ©nÃ¨re une liste d'exigences formelles basÃ©e sur le contexte projet suivant.

Contexte projet extrait des documents:
{context_summary}

Consignes :
1. **GÃ©nÃ¨re des exigences fonctionnelles et non-fonctionnelles**
2. **Formalisme :** Exigences claires, concises, non ambiguÃ«s et testables
3. **ID Unique :** Ex: EX-FON-001 (fonctionnelle), EX-NFON-001 (non-fonctionnelle)
4. **Priorisation :** Haute, Moyenne ou Basse

Format attendu pour chaque exigence:
- ID: [ID unique]
- Titre: [Nom concis]
- Description: [Exigence dÃ©taillÃ©e]
- Type: [Fonctionnelle/Non-fonctionnelle]
- PrioritÃ©: [Haute/Moyenne/Basse]

IMPORTANT: n'ajoute pas de phrase d'incitation Ã  l'interaction en fin de message
ex de ce qu'il ne faut pas ajouter: "Besoin d'ajustements ou de prÃ©cisions sur certaines exigences ?"
"""

            model = get_default_chat_model()
            messages = [
                SystemMessage(
                    content=requirements_prompt.format(context_summary=context_summary)
                )
            ]

            # Add Langfuse tracing if enabled
            langfuse_handler = self._get_langfuse_handler()
            config: RunnableConfig = (
                {"callbacks": [langfuse_handler]} if langfuse_handler else {}
            )

            response = await model.ainvoke(messages, config=config)

            # Return confirmation message
            return Command(
                update={
                    "generated_requirements": str(response.content),
                    "messages": [
                        ToolMessage(
                            "âœ“ Exigences gÃ©nÃ©rÃ©es avec succÃ¨s. Elles seront affichÃ©es Ã  la fin de la conversation.",
                            tool_call_id=runtime.tool_call_id,
                        ),
                    ],
                }
            )

        return generate_requirements

    def get_user_stories_tool(self):
        """Tool that generates user stories using a separate LLM call"""

        @tool
        async def generate_user_stories(runtime: ToolRuntime, context_summary: str):
            """
            GÃ©nÃ¨re des User Stories de haute qualitÃ© avec critÃ¨res d'acceptation exhaustifs (Gherkin).

            IMPORTANT: Avant d'appeler cet outil, utilise les outils de recherche MCP
            pour extraire les informations pertinentes des documents projet.

            Si des exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es prÃ©cÃ©demment avec generate_requirements,
            elles seront automatiquement utilisÃ©es pour assurer la cohÃ©rence.

            Args:
                context_summary: RÃ©sumÃ© du contexte projet extrait des documents

            Returns:
                Message de confirmation que les user stories ont Ã©tÃ© gÃ©nÃ©rÃ©es
            """
            stories_prompt = """
Tu es un Product Owner expert. GÃ©nÃ¨re des User Stories de haute qualitÃ©.

Contexte projet extrait des documents:
{context_summary}

{requirements_section}

**Structure de base :**
- **Format :** "En tant que [persona], je veux [action], afin de [bÃ©nÃ©fice]"
- **ID Unique :** Ex: US-001, US-002
- Stories atomiques, verticales et testables
- **CohÃ©rence :** Couvre les exigences si elles sont fournies
- **Couverture complÃ¨te :** Happy path + cas d'erreur + tous les personas

**CritÃ¨res d'Acceptation Exhaustifs (Format Gherkin)** - OBLIGATOIRE pour CHAQUE story :

1. **Cas Nominaux (Happy Path) :**
   - ScÃ©nario idÃ©al oÃ¹ tout fonctionne

2. **Validations de DonnÃ©es :**
   - Formats invalides (email, mot de passe, etc.)
   - Champs obligatoires manquants
   - Limites min/max de caractÃ¨res
   - Fichiers non supportÃ©s ou trop volumineux
   - UnicitÃ© des donnÃ©es (doublons)

3. **Cas d'Erreur :**
   - Erreurs techniques (API, timeout, erreur 500)
   - Erreurs mÃ©tier (stock insuffisant, droits insuffisants)
   - Perte de connexion

4. **Cas Limites :**
   - Valeurs frontiÃ¨res (0, 1, max, max+1)
   - Listes vides ou trÃ¨s longues
   - Dates limites (29 fÃ©vrier, changement d'heure)

5. **Feedback Utilisateur :**
   - Messages de succÃ¨s EXACTS (Toasts, Modales)
   - Messages d'erreur EXACTS affichÃ©s
   - Ã‰tats de chargement et boutons dÃ©sactivÃ©s

**Format Gherkin strict :** "Ã‰tant donnÃ© que [contexte], Quand [action], Alors [rÃ©sultat attendu]"

**MÃ©tadonnÃ©es :**
- **Estimation :** Fibonacci (1, 2, 3, 5, 8, 13, 21)
- **Priorisation :** Must Have, Should Have, Could Have, Won't Have
- **DÃ©pendances :** Ordre logique, AUCUNE dÃ©pendance circulaire
- **Questions :** 1 Ã  3 questions de clarification par story

IMPORTANT: n'ajoute pas de phrase d'incitation Ã  l'interaction en fin de message
ex de ce qu'il ne faut pas ajouter: "Besoin d'ajustements ou de prÃ©cisions sur certaines sories ?"
"""

            requirements_section = ""
            # Use stored requirements from previous tool call if available
            generated_requirements = runtime.state.get("generated_requirements")
            if generated_requirements:
                requirements_section = f"""
Exigences Ã  respecter:
{generated_requirements}
"""

            model = get_default_chat_model()
            messages = [
                SystemMessage(
                    content=stories_prompt.format(
                        context_summary=context_summary,
                        requirements_section=requirements_section,
                    )
                )
            ]

            # Add Langfuse tracing if enabled
            langfuse_handler = self._get_langfuse_handler()
            config: RunnableConfig = (
                {"callbacks": [langfuse_handler]} if langfuse_handler else {}
            )

            response = await model.ainvoke(messages, config=config)

            # Return confirmation message and update state
            return Command(
                update={
                    "generated_user_stories": str(response.content),
                    "messages": [
                        ToolMessage(
                            "âœ“ User Stories gÃ©nÃ©rÃ©es avec succÃ¨s. Elles seront affichÃ©es Ã  la fin de la conversation.",
                            tool_call_id=runtime.tool_call_id,
                        ),
                    ],
                }
            )

        return generate_user_stories

    def get_tests_tool(self):
        """Tool that generates test scenarios using a separate LLM call"""

        @tool
        async def generate_tests(runtime: ToolRuntime, jdd: str = ""):
            """
            GÃ©nÃ¨re des scÃ©narios de tests dÃ©taillÃ©s et exploitables.

            IMPORTANT: Cet outil utilise automatiquement les User Stories gÃ©nÃ©rÃ©es
            prÃ©cÃ©demment avec generate_user_stories. Assurez-vous d'avoir appelÃ©
            generate_user_stories avant d'appeler cet outil.

            Args:
                jdd: Jeu de DonnÃ©es pour les personas (optionnel, n'invente rien)

            Returns:
                Message de confirmation que les scÃ©narios de tests ont Ã©tÃ© gÃ©nÃ©rÃ©s
            """
            tests_prompt = """
## RÃ´le

Tu es un expert en tests logiciels. Ton rÃ´le est de crÃ©er des scÃ©narios de tests dÃ©taillÃ©s et exploitables.

## Instructions principales

GÃ©nÃ¨re des scÃ©narios de tests complets Ã  partir des informations fournies dans les User Stories (US) suivantes, en suivant le format Gherkin (Etant donnÃ© que-Lorsque-Alors) et en incluant les cas nominaux, limites et d'erreur. Toutes les US fournies doivent faire l'objet d'un test.
Tu peux Ã©galement te baser sur les JDDs fournis en entrÃ©e pour les personas de chaque tests

## Format de rÃ©ponse attendu ðŸ“

Pour chaque scÃ©nario :

1. **ID du ScÃ©nario** : Un identifiant unique (ex: SC-001, SC-LOGIN-001).
2. **userStoryId**: L'ID de la User Story couverte par ce test.
3. **Titre du ScÃ©nario** : Un titre concis dÃ©crivant l'objectif du test.
4. **Description** : Une brÃ¨ve explication de ce que le scÃ©nario teste.
5. **PrÃ©conditions** : Les Ã©tats ou donnÃ©es nÃ©cessaires avant l'exÃ©cution du test.
6. **Ã‰tapes** : Au format Gherkin prÃ©sentÃ©es sous forme de tableau avec les colonnes suivantes : NumÃ©ro (#1, #2, ...), Action (Etant donnÃ© que - Lorsque), RÃ©sultat attendu (Alors).
7. **DonnÃ©es de test** : Jeux de donnÃ©es nÃ©cessaires
8. **PrioritÃ©** : (Haute, Moyenne, Basse) Indiquant l'importance du test.
9. **type**: Le type de cas de test (Nominal, Limite, Erreur).

-------------------------------------------

**--- DÃ‰BUT DES USER STORIES Ã€ ANALYSER ---**
{USER_STORIES}
**--- FIN DES USER STORIES Ã€ ANALYSER ---**

**--- DÃ‰BUT DU JDD Ã€ ANALYSER ---**
{JDD}
**--- FIN DU JDD Ã€ ANALYSER ---**

IMPORTANT: n'ajoute pas de phrase d'incitation Ã  l'interaction en fin de message
ex de ce qu'il ne faut pas ajouter: "Besoin d'ajustements ou de prÃ©cisions sur certains tests ?"
"""

            # Use stored user stories from previous tool call
            generated_user_stories = runtime.state.get("generated_user_stories")
            if not generated_user_stories:
                return Command(
                    update={
                        "messages": [
                            ToolMessage(
                                "âŒ Erreur: Aucune User Story n'a Ã©tÃ© gÃ©nÃ©rÃ©e. Veuillez d'abord appeler generate_user_stories.",
                                tool_call_id=runtime.tool_call_id,
                            ),
                        ],
                    }
                )

            model = get_default_chat_model()
            messages = [
                SystemMessage(
                    content=tests_prompt.format(
                        USER_STORIES=generated_user_stories,
                        JDD=jdd if jdd else "Aucun JDD fourni",
                    )
                )
            ]

            # Add Langfuse tracing if enabled
            langfuse_handler = self._get_langfuse_handler()
            config: RunnableConfig = (
                {"callbacks": [langfuse_handler]} if langfuse_handler else {}
            )

            response = await model.ainvoke(messages, config=config)

            # Return confirmation message and update state
            return Command(
                update={
                    "generated_tests": str(response.content),
                    "messages": [
                        ToolMessage(
                            "âœ“ ScÃ©narios de tests gÃ©nÃ©rÃ©s avec succÃ¨s. Ils seront affichÃ©s Ã  la fin de la conversation.",
                            tool_call_id=runtime.tool_call_id,
                        ),
                    ],
                }
            )

        return generate_tests

    def _build_markdown_content(self, state: dict) -> str | None:
        """Build markdown content from generated requirements, user stories, and tests."""
        requirements = state.get("generated_requirements")
        user_stories = state.get("generated_user_stories")
        tests = state.get("generated_tests")

        # If nothing was generated, return None
        if not any([requirements, user_stories, tests]):
            return None

        sections = []
        sections.append("# Livrables Projet\n")
        sections.append(f"*GÃ©nÃ©rÃ© le {datetime.now().strftime('%d/%m/%Y Ã  %H:%M')}*\n")

        if requirements:
            sections.append("---\n")
            sections.append("## Exigences\n")
            sections.append(requirements)
            sections.append("\n")

        if user_stories:
            sections.append("---\n")
            sections.append("## User Stories\n")
            sections.append(user_stories)
            sections.append("\n")

        if tests:
            sections.append("---\n")
            sections.append("## ScÃ©narios de Tests\n")
            sections.append(tests)
            sections.append("\n")

        return "\n".join(sections)

    async def _generate_markdown_file(self, state: dict) -> LinkPart | None:
        """Generate a markdown file from state and return a download link."""
        content = self._build_markdown_content(state)
        if not content:
            return None

        # Create temp file with markdown content
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=".md", prefix="livrables_", mode="w", encoding="utf-8"
        ) as f:
            f.write(content)
            output_path = Path(f.name)

        # Upload to user storage
        user_id = self.get_end_user_id()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_key = f"{user_id}_livrables_{timestamp}.md"

        with open(output_path, "rb") as f_out:
            upload_result = await self.upload_user_asset(
                key=final_key,
                file_content=f_out,
                filename=f"Livrables_{timestamp}.md",
                content_type="text/markdown",
                user_id_override=user_id,
            )

        # Clean up temp file
        output_path.unlink(missing_ok=True)

        # Build download URL
        download_url = self.get_asset_download_url(
            asset_key=upload_result.key, scope="user"
        )

        return LinkPart(
            href=download_url,
            title=f"ðŸ“¥ TÃ©lÃ©charger {upload_result.file_name}",
            kind=LinkKind.download,
            mime="text/markdown",
        )

    def get_export_tool(self):
        """Tool that exports all generated deliverables to a markdown file."""

        @tool
        async def export_deliverables(runtime: ToolRuntime):
            """
            Exporte tous les livrables gÃ©nÃ©rÃ©s (exigences, user stories, tests) dans un fichier Markdown tÃ©lÃ©chargeable.

            IMPORTANT: Appelle cet outil Ã  la fin du workflow pour fournir Ã  l'utilisateur
            un fichier contenant tous les livrables gÃ©nÃ©rÃ©s.

            Returns:
                Lien de tÃ©lÃ©chargement du fichier Markdown
            """
            # Check if we have any generated content
            has_content = any(
                [
                    runtime.state.get("generated_requirements"),
                    runtime.state.get("generated_user_stories"),
                    runtime.state.get("generated_tests"),
                ]
            )

            if not has_content:
                return Command(
                    update={
                        "messages": [
                            ToolMessage(
                                "âŒ Aucun livrable n'a Ã©tÃ© gÃ©nÃ©rÃ©. Veuillez d'abord gÃ©nÃ©rer des exigences, user stories ou tests.",
                                tool_call_id=runtime.tool_call_id,
                            ),
                        ],
                    }
                )

            link_part = await self._generate_markdown_file(runtime.state)
            if link_part:
                return Command(
                    update={
                        "messages": [
                            ToolMessage(
                                content=f"âœ“ Fichier exportÃ© avec succÃ¨s: [{link_part.title}]({link_part.href})",
                                tool_call_id=runtime.tool_call_id,
                            ),
                        ],
                    }
                )

            return Command(
                update={
                    "messages": [
                        ToolMessage(
                            "âŒ Erreur lors de la gÃ©nÃ©ration du fichier.",
                            tool_call_id=runtime.tool_call_id,
                        ),
                    ],
                }
            )

        return export_deliverables

    def get_user_stories_jira_tool(self):
        """Tool that generates user stories in structured JSON format for Jira import."""

        @tool
        async def generate_user_stories_for_jira(
            runtime: ToolRuntime, context_summary: str
        ):
            """
            GÃ©nÃ¨re des User Stories dans un format structurÃ© JSON pour l'import Jira.

            Cet outil gÃ©nÃ¨re des User Stories optimisÃ©es pour l'import CSV dans Jira,
            avec tous les champs nÃ©cessaires (Summary, Description, Priority, Story Points, etc.).

            IMPORTANT: Avant d'appeler cet outil, utilise les outils de recherche MCP
            pour extraire les informations pertinentes des documents projet.

            Si des exigences ont Ã©tÃ© gÃ©nÃ©rÃ©es prÃ©cÃ©demment avec generate_requirements,
            elles seront automatiquement utilisÃ©es pour assurer la cohÃ©rence.

            Args:
                context_summary: RÃ©sumÃ© du contexte projet extrait des documents

            Returns:
                Message de confirmation que les user stories Jira ont Ã©tÃ© gÃ©nÃ©rÃ©es
            """
            stories_prompt = """Tu es un Product Owner expert. GÃ©nÃ¨re des User Stories au format JSON pour l'import Jira.

Contexte projet extrait des documents:
{context_summary}

{requirements_section}

IMPORTANT: Tu dois rÃ©pondre UNIQUEMENT avec un tableau JSON valide, sans aucun texte avant ou aprÃ¨s.

Chaque User Story doit suivre ce schÃ©ma JSON exact:
{{
  "stories": [
    {{
      "id": "US-001",
      "summary": "Titre court et descriptif de la User Story",
      "description": "En tant que [persona], je veux [action], afin de [bÃ©nÃ©fice]",
      "issue_type": "Story",
      "priority": "High|Medium|Low",
      "epic_name": "Nom de l'Epic parent (optionnel)",
      "story_points": 3,
      "labels": "label1,label2",
      "acceptance_criteria": "CritÃ¨re 1\\nCritÃ¨re 2\\nCritÃ¨re 3"
    }}
  ]
}}

RÃ¨gles:
1. **summary**: Titre concis (max 100 caractÃ¨res), format "US-XXX: Titre descriptif"
2. **description**: Format "En tant que [persona], je veux [action], afin de [bÃ©nÃ©fice]"
3. **priority**: "High" pour Must Have, "Medium" pour Should Have, "Low" pour Could Have
4. **story_points**: Fibonacci uniquement (1, 2, 3, 5, 8, 13, 21)
5. **labels**: Tags sÃ©parÃ©s par des virgules (ex: "authentication,security")
6. **acceptance_criteria**: CritÃ¨res d'acceptation en format Gherkin, sÃ©parÃ©s par \\n
7. **epic_name**: Regroupe les stories liÃ©es sous un mÃªme Epic

GÃ©nÃ¨re des User Stories couvrant:
- Cas nominaux (Happy Path)
- Validations de donnÃ©es
- Cas d'erreur
- Cas limites

RÃ©ponds UNIQUEMENT avec le JSON, sans markdown ni backticks."""

            requirements_section = ""
            generated_requirements = runtime.state.get("generated_requirements")
            if generated_requirements:
                requirements_section = f"""
Exigences Ã  respecter:
{generated_requirements}
"""

            model = get_default_chat_model()
            messages = [
                SystemMessage(
                    content=stories_prompt.format(
                        context_summary=context_summary,
                        requirements_section=requirements_section,
                    )
                )
            ]

            langfuse_handler = self._get_langfuse_handler()
            config: RunnableConfig = (
                {"callbacks": [langfuse_handler]} if langfuse_handler else {}
            )

            response = await model.ainvoke(messages, config=config)

            return Command(
                update={
                    "generated_user_stories_jira": str(response.content),
                    "messages": [
                        ToolMessage(
                            "âœ“ User Stories Jira gÃ©nÃ©rÃ©es avec succÃ¨s. Utilisez export_jira_csv pour tÃ©lÃ©charger le fichier CSV.",
                            tool_call_id=runtime.tool_call_id,
                        ),
                    ],
                }
            )

        return generate_user_stories_for_jira

    def get_export_jira_csv_tool(self):
        """Tool that exports generated Jira user stories to CSV format."""

        @tool
        async def export_jira_csv(runtime: ToolRuntime):
            """
            Exporte les User Stories gÃ©nÃ©rÃ©es pour Jira dans un fichier CSV compatible avec l'import Jira.

            IMPORTANT: Cet outil nÃ©cessite que generate_user_stories_for_jira ait Ã©tÃ© appelÃ© au prÃ©alable.

            Le fichier CSV gÃ©nÃ©rÃ© contient les colonnes standard Jira:
            - Summary, Description, IssueType, Priority, Epic Name, Epic Link, Story Points, Labels

            Note: Les critÃ¨res d'acceptation sont ajoutÃ©s Ã  la Description car ce n'est pas un champ standard Jira.

            Returns:
                Lien de tÃ©lÃ©chargement du fichier CSV
            """
            import json

            jira_data = runtime.state.get("generated_user_stories_jira")
            if not jira_data:
                return Command(
                    update={
                        "messages": [
                            ToolMessage(
                                "âŒ Aucune User Story Jira n'a Ã©tÃ© gÃ©nÃ©rÃ©e. Veuillez d'abord appeler generate_user_stories_for_jira.",
                                tool_call_id=runtime.tool_call_id,
                            ),
                        ],
                    }
                )

            # Parse JSON data
            try:
                # Clean potential markdown code blocks
                clean_data = jira_data.strip()
                if clean_data.startswith("```"):
                    clean_data = re.sub(r"^```(?:json)?\n?", "", clean_data)
                    clean_data = re.sub(r"\n?```$", "", clean_data)

                parsed = json.loads(clean_data)
                stories = parsed.get("stories", [])
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse Jira JSON: {e}")
                return Command(
                    update={
                        "messages": [
                            ToolMessage(
                                f"âŒ Erreur lors du parsing JSON: {e}. Veuillez rÃ©gÃ©nÃ©rer les User Stories.",
                                tool_call_id=runtime.tool_call_id,
                            ),
                        ],
                    }
                )

            if not stories:
                return Command(
                    update={
                        "messages": [
                            ToolMessage(
                                "âŒ Aucune User Story trouvÃ©e dans les donnÃ©es gÃ©nÃ©rÃ©es.",
                                tool_call_id=runtime.tool_call_id,
                            ),
                        ],
                    }
                )

            # Build CSV with Jira-compatible field names
            # See: https://support.atlassian.com/jira-cloud-administration/docs/import-data-from-a-csv-file/
            output = io.StringIO()
            fieldnames = [
                "Summary",
                "Description",
                "IssueType",
                "Priority",
                "Epic Name",
                "Epic Link",
                "Story Points",
                "Labels",
            ]
            writer = csv.DictWriter(
                output, fieldnames=fieldnames, quoting=csv.QUOTE_ALL
            )
            writer.writeheader()

            for story in stories:
                # Append acceptance criteria to description since it's not a standard Jira field
                description = story.get("description", "")
                acceptance_criteria = story.get("acceptance_criteria", "").replace(
                    "\\n", "\n"
                )
                if acceptance_criteria:
                    description = f"{description}\n\n*CritÃ¨res d'acceptation:*\n{acceptance_criteria}"

                writer.writerow(
                    {
                        "Summary": story.get("summary", story.get("id", "")),
                        "Description": description,
                        "IssueType": story.get("issue_type", "Story"),
                        "Priority": story.get("priority", "Medium"),
                        "Epic Name": story.get("epic_name", "")
                        if story.get("issue_type") == "Epic"
                        else "",
                        "Epic Link": story.get("epic_name", "")
                        if story.get("issue_type") != "Epic"
                        else "",
                        "Story Points": story.get("story_points", ""),
                        "Labels": story.get("labels", ""),
                    }
                )

            csv_content = output.getvalue()

            # Create temp file
            with tempfile.NamedTemporaryFile(
                delete=False,
                suffix=".csv",
                prefix="jira_import_",
                mode="w",
                encoding="utf-8",
            ) as f:
                f.write(csv_content)
                output_path = Path(f.name)

            # Upload to user storage
            user_id = self.get_end_user_id()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            final_key = f"{user_id}_jira_import_{timestamp}.csv"

            with open(output_path, "rb") as f_out:
                upload_result = await self.upload_user_asset(
                    key=final_key,
                    file_content=f_out,
                    filename=f"jira_import_{timestamp}.csv",
                    content_type="text/csv",
                    user_id_override=user_id,
                )

            # Clean up temp file
            output_path.unlink(missing_ok=True)

            # Build download URL
            download_url = self.get_asset_download_url(
                asset_key=upload_result.key, scope="user"
            )

            return Command(
                update={
                    "messages": [
                        ToolMessage(
                            content=f"âœ“ Fichier CSV Jira exportÃ© avec succÃ¨s: [{upload_result.file_name}]({download_url})\n\n"
                            f"**Pour importer dans Jira:**\n"
                            f"1. Allez dans votre projet Jira\n"
                            f"2. Menu **Project settings** > **External system import**\n"
                            f"3. SÃ©lectionnez **CSV** et uploadez le fichier",
                            tool_call_id=runtime.tool_call_id,
                        ),
                    ],
                }
            )

        return export_jira_csv

    def get_compiled_graph(self) -> CompiledStateGraph:
        requirements_tool = self.get_requirements_tool()
        user_stories_tool = self.get_user_stories_tool()
        user_stories_jira_tool = self.get_user_stories_jira_tool()
        tests_tool = self.get_tests_tool()
        export_tool = self.get_export_tool()
        export_jira_csv_tool = self.get_export_jira_csv_tool()

        return create_agent(
            model=get_default_chat_model(),
            system_prompt=self.render(self.get_tuned_text("prompts.system") or ""),
            tools=[
                requirements_tool,
                user_stories_tool,
                user_stories_jira_tool,
                tests_tool,
                export_tool,
                export_jira_csv_tool,
                *self.mcp.get_tools(),
            ],
            checkpointer=self.streaming_memory,
            state_schema=CustomState,
        )
