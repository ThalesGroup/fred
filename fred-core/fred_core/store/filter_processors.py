# Copyright Thales 2025
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Tuple

logger = logging.getLogger(__name__)


class FilterProcessor(ABC):
    """
    Abstract base class for processing structured filter dictionaries
    into backend-specific query formats.

    Handles filter fields generated by the dynamic filter system, which creates
    fields with operations like: field_name (eq), field_name__lt, field_name__gt, etc.
    """

    @abstractmethod
    def process_filters(self, filters: Dict[str, Any]) -> Any:
        """
        Process a structured filter dictionary into backend-specific format.
        
        Args:
            filters: Dictionary with filter fields like {"name__eq": "test", "count__gt": 5}
        
        Returns:
            Backend-specific query structure (e.g., OpenSearch query, SQL WHERE clause)
        """
        pass

    def _parse_filter_field(self, field_name: str) -> Tuple[str, str]:
        """
        Parse a filter field name to extract the base field and operation.
        
        Args:
            field_name: Filter field like "name", "count__gt", "tags__contains"
            
        Returns:
            Tuple of (base_field, operation)
            
        Examples:
            "name" -> ("name", "eq")
            "count__gt" -> ("count", "gt") 
            "tags__contains" -> ("tags", "contains")
        """
        if "__" in field_name:
            base_field, operation = field_name.split("__", 1)
        else:
            base_field, operation = field_name, "eq"
        
        return base_field, operation

    def _flatten_nested_filters(self, filters: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
        """
        Flatten nested filter dictionaries for processing.
        
        Args:
            filters: Potentially nested filter dictionary
            prefix: Current nesting prefix
            
        Returns:
            Flattened filter dictionary
        """
        flattened = {}
        
        for key, value in filters.items():
            current_key = f"{prefix}.{key}" if prefix else key
            
            if isinstance(value, dict) and not self._is_filter_operation(key):
                # This is a nested object, continue flattening
                flattened.update(self._flatten_nested_filters(value, current_key))
            else:
                # This is a filter operation
                flattened[current_key] = value
                
        return flattened

    def _is_filter_operation(self, key: str) -> bool:
        """Check if a key represents a filter operation (contains __ or is a direct field)."""
        return "__" in key or not isinstance(key, str)


class OpenSearchFilterProcessor(FilterProcessor):
    """
    Processes structured filters into OpenSearch query clauses.
    """

    def process_filters(self, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Convert structured filters to OpenSearch must clauses.
        
        Returns:
            List of OpenSearch query clauses for a bool/must query
        """
        if not filters:
            return []

        must_clauses = []
        flattened = self._flatten_nested_filters(filters)

        for field_name, value in flattened.items():
            if value is None:
                continue
                
            base_field, operation = self._parse_filter_field(field_name)
            clause = self._build_opensearch_clause(base_field, operation, value)
            if clause:
                must_clauses.append(clause)

        return must_clauses

    def _build_opensearch_clause(self, field: str, operation: str, value: Any) -> Dict[str, Any] | None:
        """
        Build a single OpenSearch query clause based on field, operation, and value.
        """
        if value is None:
            return None

        # Handle different field types that need special mapping
        field_mapping = self._get_field_mapping(field)
        
        match operation:
            case "eq":
                if isinstance(value, list):
                    return {"terms": {field_mapping: value}}
                else:
                    return {"term": {field_mapping: value}}
            
            case "in":
                return {"terms": {field_mapping: value if isinstance(value, list) else [value]}}
            
            case "lt":
                return {"range": {field_mapping: {"lt": value}}}
            
            case "lte":
                return {"range": {field_mapping: {"lte": value}}}
            
            case "gt":
                return {"range": {field_mapping: {"gt": value}}}
            
            case "gte":
                return {"range": {field_mapping: {"gte": value}}}
            
            case "icontains":
                return {"wildcard": {field_mapping: f"*{value}*"}}
            
            case "contains":
                # For array fields, check if value is present
                return {"term": {field: value}}
            
            case "overlap":
                # For array fields, check for any overlapping values
                return {"terms": {field: value if isinstance(value, list) else [value]}}
            
            case _:
                logger.warning(f"Unsupported filter operation: {operation}")
                return None

    def _get_field_mapping(self, field: str) -> str:
        """
        Map logical field names to OpenSearch field names.
        Some fields need special handling for keyword vs text fields.
        """
        # Fields that should use keyword mapping for exact matches
        keyword_fields = {"document_name", "title", "author", "category", "subject", "keywords"}
        
        if field in keyword_fields:
            return f"{field}.keyword"
        
        return field


class DuckDBFilterProcessor(FilterProcessor):
    """
    Processes structured filters into DuckDB SQL WHERE clauses.
    """

    def process_filters(self, filters: Dict[str, Any]) -> Tuple[str, List[Any]]:
        """
        Convert structured filters to SQL WHERE clause with parameters.
        
        Returns:
            Tuple of (WHERE clause string, parameter values list)
        """
        if not filters:
            return "", []

        where_conditions = []
        parameters = []
        flattened = self._flatten_nested_filters(filters)

        for field_name, value in flattened.items():
            if value is None:
                continue
                
            base_field, operation = self._parse_filter_field(field_name)
            condition, params = self._build_sql_condition(base_field, operation, value)
            
            if condition:
                where_conditions.append(condition)
                parameters.extend(params)

        where_clause = " AND ".join(where_conditions) if where_conditions else ""
        return where_clause, parameters

    def _build_sql_condition(self, field: str, operation: str, value: Any) -> Tuple[str, List[Any]]:
        """
        Build a SQL condition with parameters for the given field, operation, and value.
        
        Returns:
            Tuple of (condition string, parameter values)
        """
        if value is None:
            return "", []

        # Handle JSON fields specially
        if self._is_json_field(field):
            return self._build_json_condition(field, operation, value)

        match operation:
            case "eq":
                if isinstance(value, list):
                    placeholders = ",".join("?" * len(value))
                    return f"{field} IN ({placeholders})", list(value)
                else:
                    return f"{field} = ?", [value]
            
            case "in":
                values = value if isinstance(value, list) else [value]
                placeholders = ",".join("?" * len(values))
                return f"{field} IN ({placeholders})", list(values)
            
            case "lt":
                return f"{field} < ?", [value]
            
            case "lte":
                return f"{field} <= ?", [value]
            
            case "gt":
                return f"{field} > ?", [value]
            
            case "gte":
                return f"{field} >= ?", [value]
            
            case "icontains":
                return f"LOWER({field}) LIKE LOWER(?)", [f"%{value}%"]
            
            case "contains":
                # For simple text fields, treat as substring match
                return f"{field} LIKE ?", [f"%{value}%"]
            
            case "overlap":
                # For simple fields, this is equivalent to contains
                return f"{field} LIKE ?", [f"%{value}%"]
            
            case _:
                logger.warning(f"Unsupported filter operation: {operation}")
                return "", []

    def _build_json_condition(self, field: str, operation: str, value: Any) -> Tuple[str, List[Any]]:
        """
        Build SQL conditions for JSON fields (tags, processing_stages).
        """
        match operation:
            case "eq":
                if field == "tags" and isinstance(value, list):
                    # Check if all tags are present
                    conditions = []
                    params = []
                    for tag in value:
                        conditions.append("json_contains(tags, to_json(?))")
                        params.append(tag)
                    return " AND ".join(conditions), params
                else:
                    return f"json_extract({field}, '$') = to_json(?)", [value]
            
            case "in":
                values = value if isinstance(value, list) else [value]
                if field == "tags":
                    # Check if any of the values are in the tags array
                    conditions = []
                    params = []
                    for val in values:
                        conditions.append("json_contains(tags, to_json(?))")
                        params.append(val)
                    return "(" + " OR ".join(conditions) + ")", params
                else:
                    placeholders = ",".join("?" * len(values))
                    return f"json_extract({field}, '$') IN ({placeholders})", values
            
            case "contains":
                if field == "tags":
                    return "json_contains(tags, to_json(?))", [value]
                else:
                    # For other JSON fields, check if the value exists anywhere
                    return f"json_extract_string({field}, '$') LIKE ?", [f"%{value}%"]
            
            case "overlap":
                if field == "tags" and isinstance(value, list):
                    # Check if any values overlap with the tags array
                    conditions = []
                    params = []
                    for val in value:
                        conditions.append("json_contains(tags, to_json(?))")
                        params.append(val)
                    return "(" + " OR ".join(conditions) + ")", params
                else:
                    return f"json_extract_string({field}, '$') LIKE ?", [f"%{value}%"]
            
            case _:
                logger.warning(f"Unsupported JSON filter operation: {operation}")
                return "", []

    def _is_json_field(self, field: str) -> bool:
        """Check if a field is stored as JSON in the database."""
        return field in {"tags", "processing_stages"}