applicationName: knowledge-flow-backend

image:
  repository: registry.thalesdigital.io/tsn/projects/knowledge_flow_app/knowledge-flow-backend
  tag: "0.1"

Deployment:
  enabled: true

containers:
  name: knowledge-flow-backend

spec:
  revisionHistoryLimit: 2

ports:
  enabled: true
  data:
  - containerPort: 8111

rollingUpdate:
  maxSurge: 1

command:
  enabled: true
  data:
    # - /bin/sh
    # - -c
    # - tail -f /dev/null
    - python
    - /app/app/main.py
    - --config-path
    - /app/config/configuration.yaml

env:
  data:
  - name: LOG_LEVEL
    value: "INFO"

lifecycle:
  enabled: false

livenessProbe:
  enabled: false

readinessProbe:
  enabled: false

service:
  enabled: true
  Type: ClusterIP
  data:
    - name: "http"
      port: 8080
      targetPort: 8111
      protocol: TCP
    - name: "https"
      port: 8443
      targetPort: 8443
      protocol: TCP

ingress:
  enabled: true
  className: ""
  annotations: {}
  hosts:
    - host: knowledge-flow-backend.dev.fred.thalesgroup.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

volumes:
  enabled: true
  data:
  # AWS : TODO - replace the aws volume by a real AWS configuration
  - name: aws-vol
    emptyDir:
      sizeLimit: 500Mi
  # configuration.yaml && .env since we'll mount them as a directory && kube/config
  - name: "knowledge-flow-backend-vol"
    configMap:
      name: "knowledge-flow-backend-configmap"
  # kube/config
  - name: "knowledge-flow-backend-kube-vol"
    configMap:
      name: "knowledge-flow-backend-kube-configmap"
      items:
      - key: kubeconfig
        path: kubeconfig

volumeMounts:
  enabled: true
  data:
  # aws
  - name: aws-vol
    mountPath: /home/knowledge-flow-user/.aws
  # configuration.yaml && .env
  - name: knowledge-flow-backend-vol
    mountPath: /app/config
  # kube/config
  - name: knowledge-flow-backend-kube-vol
    mountPath: /home/knowledge-flow-user/.kube/config
    subPath: kubeconfig



configuration:
  # Enable or disable the security layer
  security:
    enabled: false
    keycloak_url: "http://keycloak/realms/app"
    authorized_origins:
      - "http://localhost:5173"

  # -----------------------------------------------------------------------------
  # INPUT PROCESSORS
  # -----------------------------------------------------------------------------
  # Mandatory: Input processors MUST be explicitly defined.
  # These classes parse incoming files (e.g., PDFs, DOCXs, CSVs) into structured documents.
  input_processors:
    # - prefix: ".pdf"
    #   class_path: app.core.processors.input.pdf_markdown_processor.ollama_pdf_mardown_processor.OllamaPdfMarkdownProcessor
    - prefix: ".docx"
      class_path: app.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
    - prefix: ".pptx"
      class_path: app.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
    - prefix: ".csv"
      class_path: app.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
    - prefix: ".txt"
      class_path: app.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
    - prefix: ".md"
      class_path: app.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
    - prefix: ".xlsm"
      class_path: app.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor

  # -----------------------------------------------------------------------------
  # OUTPUT PROCESSORS (Optional)
  # -----------------------------------------------------------------------------
  # Optional: You can override the default behavior for output processing.
  # If not defined, the system automatically selects based on input type:
  #   - Markdown files → VectorizationProcessor
  #   - Tabular files (CSV, XLSX) → TabularProcessor
  #
  # You can specialize behavior by mapping file extensions to custom classes.
  #
  # Example to OVERRIDE default behavior:
  #
  # output_processors:
  #   - prefix: ".docx"
  #     class_path: app.core.processors.output.vectorization_processor.vectorization_processor.VectorizationProcessor
  #   - prefix: ".csv"
  #     class_path: app.core.processors.output.tabular_processor.tabular_processor.TabularProcessor
  #   - prefix: ".xlsx"
  #     class_path: app.core.processors.output.tabular_processor.tabular_processor.TabularProcessor
  #   - prefix: ".pptx"
  #     class_path: app.core.processors.output.vectorization_processor.vectorization_processor.VectorizationProcessor
  #
  # To SKIP processing for a specific file type, you can specify an empty output processor.
  #
  # output_processors:
  #  - prefix: ".txt"
  #    class_path: app.core.processors.output.empty_output_processor.EmptyOutputProcessor
  output_processors: []

  content_storage:
  # The content store type can be either "local" or "minio" or "gcs"
  # If you are using minio, make sure to set the following environment variables:
  # - MINIO_ACCESS_KEY
  # - MINIO_SECRET_KEY
  # If you are using gcs, make sure to set the following environment variables:
  # - GCS_PROJECT_ID
  # - GCS_BUCKET_NAME
  # - GCS_CREDENTIALS_PATH
  # If you are using local storage, make sure to set the following environment variable:
  # - LOCAL_CONTENT_STORAGE_PATH default to '~/.knowledge-flow/content-store'
    type: "minio"
    endpoint: minio:9000
    # access_key: minioadmin # Overrides the MINIO_ACCESS_KEY environment variable
    # secret_key: minioadmin # Overrides the MINIO_SECRET_KEY environment variable
    secure: False
    bucket_name: app-bucket
  metadata_storage:
    type: "opensearch"
  # type: "opensearch"
  # host: https://opensearch.dev.svc.cluster.local:9200              # OpenSearch host
  # secure: true                                                     # Use HTTPS (set to false for HTTP)
  # verify_certs: false                                              # Whether to verify TLS certificates
  # metadata_index: metadata-index                                   # Index for storing metadata
  # vector_index: vector-index                                       # Vector index (required if reusing OpenSearch backend)
  # ➤ Required environment variables (not stored in this file):
  #   OPENSEARCH_USER=admin
  #   OPENSEARCH_PASSWORD=xxx
  # If you are using local storage, make sure to set the following environment variable:
  # - LOCAL_METADATA_STORAGE_PATH default to '~/.knowledge-flow/metadata-store.json'
  
  vector_storage:
    type: opensearch 
  # Available backends:
  # - in_memory  → ephemeral, dev-only (no persistence)
  # - opensearch → persistent, secure, production-ready
  # - weaviate   → persistent, fast, lightweight alternative. Still beta
  #
  # ⚠️ in_memory is used by default to ensure startup always succeeds, but it does NOT persist vectors between restarts.
  # Use it only for testing or development.
  #
  # --- Example: OpenSearch configuration
  #
  # type: opensearch
  # host: https://opensearch.dev.svc.cluster.local:9200              # OpenSearch host
  # secure: true                                                     # Use HTTPS (set to false for HTTP)
  # verify_certs: false                                              # Whether to verify TLS certificates
  # metadata_index: metadata-index                                   # Index for storing metadata
  # vector_index: vector-index                                       # Vector index (required if reusing OpenSearch backend)
  # ➤ Required environment variables (not stored in this file):
  #   OPENSEARCH_USER=admin
  #   OPENSEARCH_PASSWORD=xxx
  #
  # --- Example: Weaviate configuration
  #
  # type: weaviate
  # host: http://localhost:8080               # Weaviate HTTP host
  # index_name: CodeDocuments                 # Class name used to store chunks
  # 
  # -----------------------------------------------------------------------------
  # EMBEDDING BACKEND
  # -----------------------------------------------------------------------------
  # Set the embedding backend to use:
  #   - "openai"      → Use OpenAI's public API
  #   - "azureopenai" → Use Azure OpenAI service directly
  #   - "azureapim"   → Use Azure OpenAI via Azure APIM Gateway (OAuth2 + subscription key)
  #   - "ollama"      → Use Ollama's API
  #
  # Required environment variables based on the selected backend:
  #
  # BACKEND: "openai"
  # -------------------------------------
  # - OPENAI_API_KEY
  # - OPENAI_API_BASE (optional if using default)
  # - OPENAI_API_VERSION (optional)
  #
  # BACKEND: "azureopenai"
  # -------------------------------------
  # - AZURE_OPENAI_API_KEY
  # - AZURE_OPENAI_BASE_URL
  # - AZURE_API_VERSION
  # - AZURE_DEPLOYMENT_EMBEDDING
  #
  # BACKEND: "azureapim"
  # -------------------------------------
  # - AZURE_TENANT_ID
  # - AZURE_CLIENT_ID
  # - AZURE_CLIENT_SECRET
  # - AZURE_CLIENT_SCOPE
  # - AZURE_APIM_BASE_URL
  # - AZURE_APIM_KEY
  # - AZURE_API_VERSION
  # - AZURE_RESOURCE_PATH_EMBEDDINGS
  # - AZURE_DEPLOYMENT_EMBEDDING
  #
  # BACKEND: "ollama"
  # -------------------------------------
  # - OLLAMA_API_URL (optional)
  # - OLLAMA_EMBEDDING_MODEL_NAME
  # - OLLAMA_VISION_MODEL_NAME (optional, for vision tasks)
  #
  # All environment variables are expected to be present in the .env file
  # pointed to by the ENV_FILE variable in your Makefile.
  #
  embedding:
    type: "openai"   # can be "openai" or "azureopenai" or "azureapim" or "ollama"

  # -----------------------------------------------------------------------------
  # KNOWLEDGE CONTEXT STORAGE BACKEND
  # -----------------------------------------------------------------------------
  # Backend used to store Knowledge Contexts (Workspaces) and user profiles.
  # Both can have associated files. 
  # -------------------------------------
  knowledge_context_storage:
    type: "local"   # as of now only local storage is supported
    local_path: "~/.fred/knowledge-context"

  knowledge_context_max_tokens: 8000

dotenv:

  # -----------------------------------------------------------------------------
  # 🔵 AZURE AUTHENTICATION (for getting OAuth token)
  # -----------------------------------------------------------------------------
  AZURE_TENANT_ID: ""             # Azure Active Directory Tenant ID for your application (OAuth 2.0 flow)
  AZURE_CLIENT_ID: ""             # Client ID of your registered Azure AD Application (Service Principal)
  AZURE_CLIENT_SECRET: ""         # Client Secret of your Azure AD Application
  AZURE_CLIENT_SCOPE: ""          # OAuth2 scope for requesting tokens (typically "https://cognitiveservices.azure.com/.default")

  # -----------------------------------------------------------------------------
  # 🔵 AZURE API SETTINGS
  # -----------------------------------------------------------------------------

  AZURE_API_VERSION: "2024-06-01"           # API version used for Azure OpenAI API requests (depends on your Azure resource)

  # -----------------------------------------------------------------------------
  # 🔵 API GATEWAY (APIM) SETTINGS
  # -----------------------------------------------------------------------------
  AZURE_APIM_BASE_URL: "https://trustnest.azure-api.net"          # Base URL of your Azure API Management Gateway (APIM)
                                                                  # Example: https://company-apim-gateway.azure-api.net
  AZURE_RESOURCE_PATH_EMBEDDINGS: "/genai-aoai-inference/v1"      # Path after base URL for Embeddings API (before /deployments/...)
  AZURE_RESOURCE_PATH_LLM: "/genai-aoai-inference/v2"             # Path after base URL for LLM Chat API (before /deployments/...)
  AZURE_APIM_KEY: ""                                              # Subscription Key required by the APIM Gateway ("TrustNest-Apim-Subscription-Key" header)

  # -----------------------------------------------------------------------------
  # 🔵 AZURE OPENAI DIRECT SETTINGS (if AZURE_USE_APIM=false)
  # -----------------------------------------------------------------------------
  AZURE_OPENAI_BASE_URL: "https://your-azure-openai-resource.openai.azure.com"    # Base URL for direct Azure OpenAI access (no APIM)
  AZURE_OPENAI_API_KEY: ""                                                        # Azure OpenAI API Key (directly from Azure portal, not APIM key)

  # -----------------------------------------------------------------------------
  # 🔵 AZURE OPENAI DEPLOYMENT NAMES
  # -----------------------------------------------------------------------------
  AZURE_DEPLOYMENT_LLM: "gpt-4o"                                                  # Deployment name in Azure OpenAI for Chat LLMs (ex: GPT-4 Turbo, GPT-4o)
  AZURE_DEPLOYMENT_EMBEDDING: "fred-text-embedding-3-large"                       # Deployment name in Azure OpenAI for Embedding Models

  # -----------------------------------------------------------------------------
  # 🔵 OPENAI EMBEDDING (Public API - NOT Azure)
  # -----------------------------------------------------------------------------
  OPENAI_API_KEY: "sk-****"                     # Your OpenAI API key from https://platform.openai.com/account/api-keys
  # OPENAI_API_BASE="https://api.openai.com/v1" # Optional. Defaults to https://api.openai.com/v1 for OpenAI public API
  OPENAI_API_VERSION: ""                        # Leave blank for OpenAI public API (only needed for Azure)
                                                # Example (Azure only): "2024-06-01"

  # Example model for embeddings (default for OpenAI)
  # OPENAI_MODEL_NAME="text-embedding-ada-002"

  # -----------------------------------------------------------------------------
  # 🔵 OLLAMA SETTINGS
  # -----------------------------------------------------------------------------

  OLLAMA_API_URL: "http://localhost:11434"                          # Ollama API URL (optional)
  OLLAMA_EMBEDDING_MODEL_NAME: "snowflake-arctic-embed2:latest"     # Model name for embeddings
  OLLAMA_VISION_MODEL_NAME: "llama3-vision:latest"                  # Model name for vision tasks (optional)

  # KEYCLOAK
  KEYCLOAK_SERVER_URL: "http://keycloak"
  KEYCLOAK_REALM_NAME: "app"
  KEYCLOAK_CLIENT_ID: "app"

  # OPENSEARCH
  OPENSEARCH_USER: "admin"
  OPENSEARCH_PASSWORD: "Azerty123_"

  # MINIO
  MINIO_ACCESS_KEY: "admin"
  MINIO_SECRET_KEY: "Azerty123_"

  # GCS
  GCS_CREDENTIALS_PATH: "/path/to/sa-key.json"
  GCS_BUCKET_NAME: "my-bucket"
  GCS_PROJECT_ID: "my-gcp-project"

  # LOCAL STORAGE
  LOCAL_CONTENT_STORAGE_PATH: "~/.knowledge-flow/content-store"
  LOCAL_METADATA_STORAGE_PATH: "~/.knowledge-flow/metadata-store.json"

kubeconfig:
  data:
    kubeconfig: |
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority-data: *****
          extensions:
          - extension:
              last-update: Tue, 10 Jun 2025 11:21:54 CEST
              provider: minikube.sigs.k8s.io
              version: v1.35.0
            name: cluster_info
          server: https://yyy.yyy.yyy.yyy:8443
        name: minikube
      contexts:
      - context:
          cluster: minikube
          extensions:
          - extension:
              last-update: Tue, 10 Jun 2025 11:21:54 CEST
              provider: minikube.sigs.k8s.io
              version: v1.35.0
            name: context_info
          namespace: default
          user: minikube
        name: minikube
      current-context: minikube
      kind: Config
      preferences: {}
      users:
      - name: minikube
        user:
          client-certificate-data: kkkkkk
          client-key-data: mmmmmmm