# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

app:
  name: "Knowledge Flow Backend"
  base_url: "/knowledge-flow/v1"
  address: "127.0.0.1"
  port: 8111
  log_level: "info"
  reload: false
  reload_dir: "."
  metrics_enabled: false
  metrics_address: "0.0.0.0"
  metrics_port: 9111
  kpi_process_metrics_interval_sec: 10

processing:
  generate_summary: false
  use_gpu: true
  process_images: false

security:
  m2m:
    enabled: true
    client_id: "knowledge-flow"
    realm_url: "http://app-keycloak:8080/realms/app"
    secret_env_var: KEYCLOAK_KNOWLEDGE_FLOW_CLIENT_SECRET # pragma: allowlist secret
  user:
    enabled: true
    client_id: "app"
    realm_url: "http://app-keycloak:8080/realms/app"
  authorized_origins:
    - "http://localhost:5173"
  rebac:
    enabled: true
    type: openfga
    api_url: "http://localhost:9080"

scheduler:
  enabled: true
  backend: "memory"
  temporal:
    host: "localhost:7233"
    namespace: "default"
    task_queue: "ingestion"
    workflow_prefix: "pipeline"
    connect_timeout_seconds: 5

input_processors:
  - prefix: ".pdf"
    class_path: knowledge_flow_backend.core.processors.input.pdf_markdown_processor.pdf_markdown_processor.PdfMarkdownProcessor
  - prefix: ".docx"
    class_path: knowledge_flow_backend.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
  - prefix: ".pptx"
    class_path: knowledge_flow_backend.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
  - prefix: ".csv"
    class_path: knowledge_flow_backend.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
  - prefix: ".txt"
    class_path: knowledge_flow_backend.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
  - prefix: ".md"
    class_path: knowledge_flow_backend.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
  - prefix: ".xlsm"
    class_path: knowledge_flow_backend.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor
  - prefix: ".jsonl"
    class_path: knowledge_flow_backend.core.processors.input.jsonl.jsonl_markdown_processor.JsonlMarkdownProcessor

# Optional: fast processors used for chat attachments (/fast/text, /fast/ingest).
# If omitted, defaults to the Unstructured fast processor.
attachment_processors:
  - prefix: "*"
    class_path: knowledge_flow_backend.core.processors.input.fast_text_processor.fast_unstructured_text_processor.FastUnstructuredTextProcessingProcessor

content_storage:
  type: minio
  endpoint: http://localhost:9000
  access_key: "admin"
  bucket_name: "knowledge-flow-content"
  secure: false

document_sources:
  fred:
    type: push
    description: "Documents manually uploaded by users"

chat_model:
  provider: openai
  name: gpt-4o-mini # any chat-capable model (gpt-4o, gpt-4o-mini, gpt-4.1, etc.)
  settings:
    temperature: 0 # optional, forwarded to LangChain wrapper
    max_retries: 2 # optional
embedding_model:
  provider: openai
  name: text-embedding-3-large # or text-embedding-3-small
  settings: {}
vision_model:
  provider: openai
  name: gpt-4o-mini # or gpt-4o if you want higher fidelity
  settings:
    temperature: 0 # optional
    max_retries: 2 # optional
crossencoder_model:
  name: cross-encoder/ms-marco-MiniLM-L-12-v2
  settings:
    online: true
    local_path: ~/.cache/huggingface/hub

# chat_model:
#   provider: ollama
#   name: qwen2.5:3b-instruct         # any chat-capable Ollama model you pulled
#   settings:
#     base_url: http://localhost:11434
#     temperature: 0.7                # optional, forwarded to LangChain Ollama wrapper

# embedding_model:
#   provider: ollama
#   name: nomic-embed-text            # embedding model served by Ollama
#   settings:
#     base_url: http://localhost:11434

# chat_model:
#   provider: azure
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-gpt-4o              # Azure deployment name for your chat model
#   settings:
#     azure_endpoint: https://<your-azure-openai>.openai.azure.com
#     azure_openai_api_version: 2024-05-01-preview
# embedding_model:
#   provider: azure
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-embed-3             # Azure deployment name for your embedding model
#   settings:
#     azure_endpoint: https://<your-azure-openai>.openai.azure.com
#     azure_openai_api_version: 2024-05-01-preview

# embedding_model:
#   provider: azure-apim
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-text-embedding-3-large
#   settings:
#     azure_ad_client_id: "your-client-id"
#     azure_ad_client_scope: "api://your-client-id/.default"
#     azure_apim_base_url: "https://trustnest.azure-api.net"
#     azure_apim_resource_path: "/genai-aoai-inference/v1"
#     azure_openai_api_version: "2024-06-01"
#     azure_tenant_id: "your-tenant-id"

# chat_model:
#   provider: azure-apim        # openai | azure | ollama
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: gpt-4o
#   settings:
#     azure_ad_client_id: "your-client-id"
#     azure_ad_client_scope: "api://your-client-id/.default"
#     azure_apim_base_url: "https://trustnest.azure-api.net"
#     azure_apim_resource_path: "/genai-aoai-inference/v2"
#     azure_openai_api_version: "2024-06-01"
#     azure_tenant_id: "your-tenant-id"

storage:
  postgres:
    host: localhost
    port: 5432
    database: fred
    username: fred

  opensearch:
    host: https://localhost:9200
    secure: true
    verify_certs: false
    username: admin

  catalog_store:
    type: "postgres"
    table: catalog

  prompt_store:
    type: "postgres"
    table: prompt

  resource_store:
    type: "postgres"
    table: resource

  kpi_store:
    type: "opensearch"
    index: kpi-index

  tag_store:
    type: "postgres"
    table: tag

  group_store:
    type: "postgres"
    table: group_profile

  metadata_store:
    type: "postgres"
    table: metadata

  vector_store:
    type: opensearch
    index: vector-index
    # Default bulk size for vector insertions is 500. Increase if you have
    # enough memory and want faster ingestion. Or if run into ingestion errors
    # while ingesting large documents.
    bulk_size: 1000

  log_store:
    type: "opensearch"
    index: log-index

  tabular_stores:
    base_database: # Minimal configuration to enable CSV document ingestion
      type: "sql"
      driver: "duckdb"
      database: "base_database"
      path: "~/.fred/knowledge-flow/db.duckdb"
      mode: "read_and_write"

mcp:
  reports_enabled: true
  kpi_enabled: true
  tabular_enabled: true
  statistic_enabled: true
  text_enabled: true
  templates_enabled: true
  resources_enabled: true
  opensearch_ops_enabled: true
  neo4j_enabled: false
  filesystem_enabled: false

filesystem:
  type: minio
  endpoint: http://localhost:9000
  access_key: admin
  bucket_name: filesystem
  secure: false
