# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

app:
  name: "Knowledge Flow Backend"
  base_url: "/knowledge-flow/v1"
  address: "127.0.0.1"
  port: 8111
  log_level: "info"
  reload: false
  reload_dir: "."

processing:
  generate_summary: true
  use_gpu: true
  process_images: true

security:
  m2m:
    enabled: true
    client_id: "knowledge-flow"
    realm_url: "http://app-keycloak:8080/realms/app"
  user:
    enabled: true
    client_id: "app"
    realm_url: "http://app-keycloak:8080/realms/app"
  authorized_origins:
    - "http://localhost:5173"

scheduler:
  enabled: true
  backend: "temporal"
  temporal:
    host: "localhost:7233"
    namespace: "default"
    task_queue: "ingestion"
    workflow_prefix: "pipeline"
    connect_timeout_seconds: 5

input_processors:
  - prefix: ".pdf"
    class_path: app.core.processors.input.pdf_markdown_processor.pdf_markdown_processor.PdfMarkdownProcessor
  - prefix: ".docx"
    class_path: app.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
  - prefix: ".pptx"
    class_path: app.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
  - prefix: ".csv"
    class_path: app.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
  - prefix: ".txt"
    class_path: app.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
  - prefix: ".md"
    class_path: app.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
  - prefix: ".xlsm"
    class_path: app.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor
  - prefix: ".jsonl"
    class_path: app.core.processors.input.jsonl.jsonl_markdown_processor.JsonlMarkdownProcessor

content_storage:
  type: minio
  endpoint: http://localhost:9000
  access_key: "admin"
  bucket_name: "knowledge-flow-content"
  secure: false

document_sources:
  fred:
    type: push
    description: "Documents manually uploaded by users"

  local:
    type: pull
    provider: local_path
    base_path: ~/Documents/Fred
    description: "Personal local documents available for pull-mode ingestion"

  minio:
    type: pull
    provider: minio
    description: "Documents available for pull from MinIO"
    endpoint_url: localhost:9000
    bucket_name: "shared-content"
    prefix: ""
    access_key: "admin"
    secure: false

chat_model:
  provider: openai
  name: gpt-4o-mini # any chat-capable model (gpt-4o, gpt-4o-mini, gpt-4.1, etc.)
  settings:
    temperature: 0 # optional, forwarded to LangChain wrapper
    max_retries: 2 # optional
embedding_model:
  provider: openai
  name: text-embedding-ada-002 # or text-embedding-3-small
  settings: {}
vision:
  provider: openai
  name: gpt-4o-mini # or gpt-4o if you want higher fidelity
  settings:
    temperature: 0 # optional
    max_retries: 2 # optional

# chat_model:
#   provider: ollama
#   name: qwen2.5:3b-instruct         # any chat-capable Ollama model you pulled
#   settings:
#     base_url: http://localhost:11434
#     temperature: 0.7                # optional, forwarded to LangChain Ollama wrapper

# embedding_model:
#   provider: ollama
#   name: nomic-embed-text            # embedding model served by Ollama
#   settings:
#     base_url: http://localhost:11434

# chat_model:
#   provider: azure
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-gpt-4o              # Azure deployment name for your chat model
#   settings:
#     azure_endpoint: https://<your-azure-openai>.openai.azure.com
#     azure_openai_api_version: 2024-05-01-preview
# embedding_model:
#   provider: azure
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-embed-3             # Azure deployment name for your embedding model
#   settings:
#     azure_endpoint: https://<your-azure-openai>.openai.azure.com
#     azure_openai_api_version: 2024-05-01-preview

# embedding_model:
#   provider: azure-apim
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-text-embedding-3-large
#   settings:
#     azure_ad_client_id: "your-client-id"
#     azure_ad_client_scope: "api://your-client-id/.default"
#     azure_apim_base_url: "https://trustnest.azure-api.net"
#     azure_apim_resource_path: "/genai-aoai-inference/v1"
#     azure_openai_api_version: "2024-06-01"
#     azure_tenant_id: "your-tenant-id"

# chat_model:
#   provider: azure-apim        # openai | azure | ollama
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: gpt-4o
#   settings:
#     azure_ad_client_id: "your-client-id"
#     azure_ad_client_scope: "api://your-client-id/.default"
#     azure_apim_base_url: "https://trustnest.azure-api.net"
#     azure_apim_resource_path: "/genai-aoai-inference/v2"
#     azure_openai_api_version: "2024-06-01"
#     azure_tenant_id: "your-tenant-id"

storage:
  postgres:
    host: localhost
    port: 5432
    database: fred
    username: admin

  opensearch:
    host: https://localhost:9200
    secure: true
    verify_certs: false
    username: admin

  catalog_store:
    type: "opensearch"
    index: catalog-index

  prompt_store:
    type: "opensearch"
    index: prompt-index

  resource_store:
    type: "opensearch"
    index: resource-index

  kpi_store:
    type: "opensearch"
    index: kpi-index

  tag_store:
    type: "opensearch"
    index: tag-index

  metadata_store:
    type: "opensearch"
    index: metadata-index

  vector_store:
    type: opensearch
    index: vector-index-ada002
    # Default bulk size for vector insertions is 500. Increase if you have
    # enough memory and want faster ingestion. Or if run into ingestion errors
    # while ingesting large documents.
    bulk_size: 1000

  log_store:
    type: "opensearch"
    index: log-index

  tabular_stores:
    base_database: # Minimal configuration to enable CSV document ingestion
      type: "sql"
      driver: "duckdb"
      database: "base_database"
      path: "~/.fred/knowledge-flow/db.duckdb"
      mode: "read_and_write"
